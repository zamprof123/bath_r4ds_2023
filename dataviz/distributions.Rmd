---
title: "Distributions"
author: "Rafael A. Irizarry"
date: "`r lubridate::today()`"
output:
  ioslides_presentation:
    fig_caption: no
    fig_height: 6
    fig_width: 7
  beamer_presentation: default
  slidy_presentation: default
---

```{r setup, include=FALSE}
library(tidyverse)
library(dslabs)
ds_theme_set()
options(digits = 3)
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  cache = TRUE,
  out.width = "70%",
  fig.align = 'center',
  fig.width = 6,
  fig.asp = 0.618,  # 1 / phi
  fig.show = "hold"
)
```

#  Distributions and Summary Statistics


## Variable types

* categorical data

* ordinal

* numerical: Some numerical data can be treated as ordered categorical.

  - continuous or discrete


## Distributions 

```{r summaries-state-region-distribution}
prop.table(table(state.region))
```


## Case study: describing student heights 

```{r summaries-load-heights, warning=FALSE, message=FALSE}
library(tidyverse)
library(dslabs)
data(heights)
```

## Case study: describing student heights 


* One way to provide a summary of heights to ET is to simply send him this list of `r nrow(heights)` heights. 

* But there are much more effective ways to convey this information, and understanding the concept of a distribution will help. 

## Empirical cumulative distribution functions 

* Statistics textbooks teach us that a more useful way to define a distribution for numeric data is to define a function that reports the proportion of the data entries $x$ that are below $a$, for all possible values of $a$. 

## Empirical cumulative distribution functions 

* This function is called the empirical cumulative distribution function (eCDF) and often denoted with $F$:


$$ F(a) = \mbox{Proportion of data points that are less than or equal to }a$$

## Empirical cumulative distribution functions 

Here is a plot of $F$ for the male height data:

```{r summaries-ecdf, echo=FALSE}
ds_theme_set()
heights |> filter(sex=="Male") |> ggplot(aes(height)) + 
  stat_ecdf() +
  ylab("F(a)") + xlab("a")
```

## Empirical cumulative distribution functions 

* Similar to what the frequency table does for categorical data, the eCDF defines the distribution for numerical data. From the plot, we can see that `r round(ecdf(heights$height[heights$sex=="Male"])(66)*100)`% 
of the values are below 65, since 
$F(66)=$ `r ecdf(heights$height[heights$sex=="Male"])(66)`, 
or that `r round(ecdf(heights$height[heights$sex=="Male"])(72)*100)`% 
of the values are below 72, since 
$F(72)=$ `r ecdf(heights$height[heights$sex=="Male"])(72)`, 
and so on. 

## Empirical cumulative distribution functions 

* In fact, we can report the proportion of values between any two heights, say $a$ and $b$, by computing $F(b) - F(a)$. This means that if we send this plot, we will have all the information needed to reconstruct the entire list. 

* Note: the reason we add the word _empirical_ is because, as we will see in \@ref(cdf-intro), the cumulative distribution function (CDF) can be defined mathematically, meaning without any data.


## Histograms
 
 
Here is the histogram for the height data splitting the range of values into one inch intervals: $(49.5, 50.5],(50.5, 51.5],(51.5,52.5],(52.5,53.5],...,(82.5,83.5]$
  
```{r summaries-height-histogram, echo=FALSE}
heights |> 
  filter(sex=="Male") |> 
  ggplot(aes(height)) + 
  geom_histogram(binwidth = 1, color = "black")
```

## Histograms

If we send this plot as a summary , we will immediately learn some important properties about our data. 

1. The range of the data is from 50 to 84 with the majority (more than 95%) between 63 and 75 inches. 

2. The heights are close to symmetric around 69 inches. 

By adding up counts, we could obtain a very good approximation of the proportion of the data in any interval. 

But, what information do we lose?  

## Smoothed density 

Smooth density plots are similar to histograms, but the data is not divided into bins. Here is what a smooth density plot looks like for our heights data:

```{r summaries-example-of-smoothed-density, echo=FALSE}
heights |> 
  filter(sex=="Male") |> 
  ggplot(aes(height)) + 
  geom_density(alpha = .2, fill= "#00BFC4", color = 0)  +
  geom_line(stat='density')
```

## Smoothed density 


```{r summaries--simulated-data-histogram-1, echo=FALSE}
set.seed(1988)
x <- data.frame(height = c(rnorm(1000000,69,3), rnorm(1000000,65,3)))
x |> ggplot(aes(height)) + geom_histogram(binwidth = 1, color = "black")
```

## Smoothed density 

The smaller we make the bins, the smoother the histogram gets. Here are the histograms with bin width of 1, 0.5, and 0.1:

```{r summaries-simulated-data-histogram-2, fig.width=9, fig.height=3,  out.width = "100%",echo=FALSE, message=FALSE}
p1 <- x |> ggplot(aes(height)) + geom_histogram(binwidth = 1) + ggtitle("binwidth=1")
p2 <- x |> ggplot(aes(height)) + geom_histogram(binwidth = 0.5) + ggtitle("binwidth=0.5") 
p3 <- x |> ggplot(aes(height)) + geom_histogram(binwidth = 0.1) + ggtitle("binwidth=0.1")
library(gridExtra)
grid.arrange(p1, p2, p3, nrow = 1)
```

## Smoothed density 

The smooth density is basically the curve that goes through the top of the histogram bars when the bins are very, very small. :

```{r, simulated-density-1, echo=FALSE}
x |> ggplot(aes(height)) + 
  geom_histogram(aes(y=..density..), binwidth = 0.1) +
  geom_line(stat='density')
```

To make the curve not depend on the hypothetical size of the hypothetical list, we compute the curve on frequencies rather than counts.


## Smoothed density 

Now, back to reality. We don't have millions of measurements. Instead, we have `r sum(heights$sex=="Male")` and we can't make a histogram with very small bins. 

We therefore make a histogram, using bin sizes appropriate for our data and computing frequencies rather than counts, and we draw a smooth curve that goes through the tops of the histogram bars. 

## Smoothed density 

```{r summaries-smooth-density-2, echo=FALSE, out.width = "100%"}
hist1 <- heights |> 
  filter(sex=="Male") |> 
  ggplot(aes(height)) +
  geom_histogram(aes(y=..density..), binwidth = 1, color="black") 
hist2 <- hist1 +
  geom_line(stat='density')
hist3 <- hist1 + 
  geom_point(data = ggplot_build(hist2)$data[[1]], aes(x,y), col = "blue")
hist4 <- ggplot() + geom_point(data = ggplot_build(hist2)$data[[1]], aes(x,y), col = "blue") + 
  xlab("height") + ylab("density")
hist5 <- hist4 + geom_line(data = ggplot_build(hist2)$data[[2]], aes(x,y))
hist6 <- heights |> 
  filter(sex=="Male") |> 
  ggplot(aes(height)) +
  geom_density(alpha = 0.2, fill="#00BFC4", col = 0) +
  geom_line(stat='density') +
  scale_y_continuous(limits = layer_scales(hist2)$y$range$range)
  
grid.arrange(hist1, hist3, hist4, hist5, hist2, hist6, nrow=2)
```

## Smoothed density 


However, remember that _smooth_ is a relative term. We can actually control the _smoothness_ of the curve that defines the smooth density through an option in the function that computes the smooth density curve. 

## Smoothed density 

Here are two examples using different degrees of smoothness on the same histogram:


```{r summaries-densities-different-smoothness, echo = FALSE, out.width = "100%", fig.width = 6, fig.height = 3}
p1 <- heights |> 
  filter(sex=="Male")|> ggplot(aes(height)) +
  geom_histogram(aes(y=..density..), binwidth = 1, alpha = 0.5) + 
  geom_line(stat='density', adjust = 0.5)

p2 <- heights |> 
  filter(sex=="Male") |> ggplot(aes(height)) +
  geom_histogram(aes(y=..density..), binwidth = 1, alpha = 0.5) + 
  geom_line(stat='density', adjust = 2)

grid.arrange(p1,p2, ncol=2)
```

## Smoothed density 

& We need to make this choice with care as the resulting summary can change our interpretation of the data. We should select a degree of smoothness that we can defend as being representative of the underlying data. 

* While the histogram is an assumption-free summary, the smoothed density is based on some assumptions. 

* Note that interpreting the y-axis of a smooth density plot is not straightforward. It is scaled so that the area under the density curve adds up to 1. 

## Smoothed density 

The proportion of this area is about 
`r round(mean(dplyr::between(heights$height[heights$sex=="Male"], 65, 68)), 2)`, 
meaning that about 
`r noquote(paste0(round(mean(dplyr::between(heights$height[heights$sex=="Male"], 65, 68)), 2)*100, '%'))`
of male heights are between 65 and 68 inches.

```{r summaries-area-under-curve, echo=FALSE}
d <- with(heights, density(height[sex=="Male"]))
tmp <- data.frame(height=d$x, density=d$y)
tmp |> ggplot(aes(height,density)) + geom_line() + 
  geom_area(aes(x=height,y=density), data = filter(tmp, between(height, 65, 68)), alpha=0.2, fill="#00BFC4")
```

## Smoothed density 

By understanding this, we are ready to use the smooth density as a summary. For this dataset, we would feel quite comfortable with the smoothness assumption, and therefore with sharing an aesthetically pleasing figure, which he could use to understand our male heights data.

## Smoothed density 


```{r summaries-example-of-smoothed-density-2, echo=FALSE}
heights |> 
  filter(sex=="Male") |> 
  ggplot(aes(height)) + 
  geom_density(alpha=.2, fill= "#00BFC4", color = 0)  +
  geom_line(stat='density')
```

## Break for exercises

## The normal distribution 

$$\mbox{Pr}(a < x \leq b) = \int_a^b \frac{1}{\sqrt{2\pi}s} e^{-\frac{1}{2}\left( \frac{x-m}{s} \right)^2} \, dx$$

* Note that it is completely defined by just two parameters: $m$ and $s$, refered to as average and standard deviation.

* The distribution is symmetric, centered at the average

* Most values (about 95%) are within 2 SDs from the average. 

## The normal distribution 

Here is what the normal distribution looks like when the average is 0 and the SD is 1:

```{r summaries-normal-distribution-density, echo=FALSE}
mu <- 0; s <- 1
norm_dist <- data.frame(x=seq(-4,4,len=50)*s+mu) |> mutate(density=dnorm(x,mu,s))
norm_dist |> ggplot(aes(x,density)) + geom_line() + ylab("f(x)")
```

## The normal distribution 

* The fact that the distribution is defined by just two parameters implies that if a dataset is approximated by a normal distribution, all the information needed to describe the distribution can be encoded in just two numbers: 

* the average $m$ and 

* the standard deviation $s$.

## The normal distribution 

For a list of numbers contained in a vector `x`, the average is defined as: 

```{r, eval=TRUE}
m <- sum(x) / length(x)
```

and the SD is defined as:
```{r}
s <- sqrt(sum((x-mu)^2) / length(x))
```
which can be interpreted as the average distance between values and their average. 

## The normal distribution 

Let's compute the values for the height for males which we will store in the object $x$:

```{r}
index <- heights$sex == "Male"
x <- heights$height[index]
```

## The normal distribution 

The pre-built functions `mean` and `sd`  can be used here:
```{r}
m <- mean(x)
s <- sd(x)
c(average = m, sd = s)
```

## The normal distribution 

Advacned note: for reasons explained in statistics textbooks, the function `sd` is an estimate of an SD rather than an SD, defined by 

```{r eval=FALSE}
s <- sqrt(sum((x-mu)^2) / (length(x) - 1))
```

Note it divides by `length(x)-1` rather than `length(x)`. 

But `sd(x)` and 
`sqrt(sum((x-mu)^2) / length(x))` are are practically equal when `length(x)` is large, so we often use `sd(x)` when we really want `sqrt(sum((x-mu)^2) / length(x))`.


## Example

Here is a plot of the smooth density of male heights in blue and the normal distribution with mean  = `r round(m,1)` and SD = `r round(s,1)` plotted as a black curve:

```{r summaries-data-and-normal-densities, echo=FALSE}
norm_dist <- data.frame(x = seq(-4, 4, len=50)*s + m) |> 
  mutate(density = dnorm(x, m, s))

heights |> filter(sex == "Male") |> ggplot(aes(height)) +
  geom_density(fill="#0099FF") +
  geom_line(aes(x, density),  data = norm_dist, lwd=1.5) 
```

## Standard units 

* Standard units for a vector `x` is defined as the nnumber of SDs  `(x - mean(x)) / sd(x)`. 

* It tells us, for each entry of `x`, how many SDs away from the average it is.

* Look back at the formula for the normal distribution and note that what is being exponentiated is $-z^2/2$ with $z$ equivalent to $x$ in standard units. 

* Because the maximum of  $e^{-z^2/2}$ is when $z=0$, this explains why the maximum of the distribution occurs at the average. It also explains the symmetry since $- z^2$ is symmetric around 0. 

## Standard units 

* Second, note that if we convert the normally distributed data to standard units, we can quickly know if, for example, 
   - a person is about average ($z \approx 0$), 
   - one of the largest ($z \approx 2$), 
   - one of the smallest ($z \approx -2$), 
   - or an extremely rare occurrence ($z > 3$ or $z < -3$). 

* Remember that it does not matter what the original units are, these rules apply to any data that is approximately normal.

## Standard units 

In R, we can obtain standard units using the function `scale`:
```{r}
z <- scale(x)
```

## Standard units 

Now to see how many men are within 2 SDs from the average, we simply type:

```{r}
mean(abs(z) < 2)
```

The proportion is about 95%, which is what the normal distribution predicts! To further confirm that, in fact, the approximation is a good one, we can use quantile-quantile plots.


## Quantile-quantile plots

* First let's define the theoretical quantiles for the normal distribution.

* In statistics books we use the symbol $\Phi(x)$ to define the function that gives us the proportion of a standard normal distributed data that are smaller than $x$.

* So, for example, since 95% are between 2 and 2, $\Phi(-2) \approx 0.025$ and $\Phi(2) \approx 0.975$. 

* In R, we can evaluate $\Phi$ using the `pnorm` function:

```{r}
pnorm(-2)
```

* Note: To get exactly 0.95 we use `pnorm(-1.96)`

## Quantile-quantile plots

* The inverse function $\Phi^{-1}(x)$ gives us the _theoretical quantiles_ for the normal distribution. 

* So, for example, $\Phi^{-1}(0.975) = 1.96$. 

* In R, we can evaluate the inverse of $\Phi$ using the `qnorm` function.

```{r}
qnorm(0.975)
```

## Quantile-quantile plots

* Note that these calculations are for the standard normal distribution by default (mean = 0, standard deviation = 1), 

* but we can also define these for any normal distribution. 

```{r}
qnorm(0.975, mean = 5, sd = 2)
```

## Quantile-quantile plots

* For the normal distribution, all the calculations related to quantiles are done without data, thus the name _theoretical quantiles_. 

* But quantiles can be defined for any distribution, including an empirical one. 

* So if we have data in a vector $x$, we can define the quantile associated with any proportion $p$ as the $q$ for which the proportion of values below $q$ is $p$. 

## Quantile-quantile plots

* We can define `q` as the value for which `mean(x <= q) = p`

* Notice that not all $p$ have a $q$ for which the proportion is exactly $p$. 

* There are several ways of defining the best $q$ as discussed in the help for the `quantile` function. 

## Quantile-quantile plots

To give a quick example, for the male heights data, we have that:

```{r}
mean(x <= 69.5)
```

So about 50% are shorter or equal to 69.5 inches. This implies that if $p=0.50$ then $q=69.5$.

## Quantile-quantile plots

* The idea of a QQ-plot is that if your data is well approximated by normal distribution then the quantiles of your data should be similar to the quantiles of a normal distribution. 

## Quantile-quantile plots

To construct a QQ-plot, we do the following:

1. Define a vector of $m$ proportions $p_1, p_2, \dots, p_m$.

2. Define a vector of quantiles $q_1, \dots, q_m$ for your data for the proportions $p_1, \dots, p_m$. We refer to these as the _sample quantiles_. 

3. Define a vector of theoretical quantiles for the proportions $p_1, \dots, p_m$ for a normal distribution with the same average and standard deviation as the data.

4. Plot the sample quantiles versus the theoretical quantiles.

## Quantile-quantile plots

Let's construct a QQ-plot using R code. Start by defining the vector of proportions.
```{r}
p <- seq(0.05, 0.95, 0.05)
```

## Quantile-quantile plots

To obtain the quantiles from the data, we can use the `quantile` function like this:
```{r}
sample_quantiles <- quantile(x, p)
```

## Quantile-quantile plots

To obtain the theoretical normal distribution quantiles with the corresponding average and SD, we use the `qnorm` function:
```{r}
theoretical_quantiles <- qnorm(p, mean = mean(x), sd = sd(x))
```

## Quantile-quantile plots

To see if they match or not, we plot them against each other and draw the identity line:

```{r summaries-qqplot-original}
qplot(theoretical_quantiles, sample_quantiles) + geom_abline()
```

## Quantile-quantile plots

Notice that this code becomes much cleaner if we use standard units:
```{r summaries-qqplot-standardized, eval=FALSE}
z <- scale(x)
sample_quantiles <- quantile(z, p)
theoretical_quantiles <- qnorm(p) 
qplot(theoretical_quantiles, sample_quantiles) + geom_abline()
```

## Quantile-quantile plots

The code we just showed is included to help describe QQ-plots. 

However, in practice it is easier to use **ggplot2** code:

```{r, eval=FALSE}
heights |> filter(sex == "Male") |>
  ggplot(aes(sample = scale(height))) + 
  geom_qq() +
  geom_abline()
```

## Quantile-quantile plots

While for the illustration above we used 20 quantiles, the default from the `geom_qq` function is to use as many quantiles as data points.

```{r, echo=FALSE}
heights |> filter(sex == "Male") |>
  ggplot(aes(sample = scale(height))) + 
  geom_qq() +
  geom_abline()
```

## Quantile-quantile plots
  
Note that although here we used qqplots to compare an observed distribution to the mathamatically defeine normal distribution, QQ-plots can be used to compare any two distributions.

## Percentiles 

* Before we move on, let's define some terms that are commonly used in exploratory data analysis.

* The percentiles are the quantiles you obtain when setting the $p$ at $0.01, 0.02, ..., 0.99$. 

* We call, for example, the case of $p=0.25$ the 25th percentile, which gives us a number for which 25% of the data is below. The most famous percentile is the 50th, also known as the _median_. 

## Percentiles 

For the normal distribution the _median_ and average are the same, but this is generally not the case.

Another special case that receives a name are the _quartiles_, which are obtained when setting $p=0.25,0.50$, and $0.75$. 


## Boxplots

* Suppose those used to receiving just two numbers as summaries ask us for a more compact numerical summary, rather than histogram or smooth density.

* The boxplot provides a five-number summary composed of the range along with the quartiles (the 25th, 50th, and 75th percentiles). 


## Boxplots

The US state murder rates are an example of a variable that does not follow a normal distribution:

```{r summaries-hist-qqplot-non-normal-data, out.width = "90%",  fig.width = 6, fig.height = 3, echo=FALSE}
data(murders)
murders <- murders |> mutate(rate = total/population*100000)
library(gridExtra)
p1 <- murders |> ggplot(aes(x=rate)) + geom_histogram(binwidth = 0.5, color = "black") + ggtitle("Histogram")
p2 <- murders |> ggplot(aes(sample=rate)) + 
  geom_qq(dparams=summarize(murders, mean=mean(rate), sd=sd(rate))) +
  geom_abline() + ggtitle("QQ-plot")
grid.arrange(p1, p2, ncol = 2)
```


## Boxplots

While the average and SD two number summary is not useful, the 5 number boxplot tells use quite a bit:

```{r summaries-first-boxplot, echo=FALSE}
murders |> ggplot(aes("",rate)) + geom_boxplot() +
  coord_cartesian(xlim = c(0, 2)) + xlab("")
```

## Stratification 

* In data analysis we often divide observations into groups based on the values of one or more variables associated with those observations.  

* We call this procedure _stratification_ and refer to the resulting groups as _strata_. 

## Stratification 

* Stratification is common in data visualization because we are often interested in how the distributions of variables differ across different subgroups. 

* We will see several examples in the homework are future lectures.



## Case study: describing student heights 

Here is an example with student heights:

```{r summaries-female-male-boxplots, echo=FALSE}
heights |> ggplot(aes(x=sex, y=height, fill=sex)) +
  geom_boxplot()
```

## Case study: describing student heights 

* We saw that the normal approximation worked well for the male heights in our dataset.

* Now we see if it works for the female heights in our dataset.

* Our first exploratory plots reveal that the approximation is not as useful:
 

```{r summaries-histogram-qqplot-female-heights, echo=FALSE, out.width="90%",  fig.width = 6, fig.asp = 0.4}
p1 <- heights |> filter(sex == "Female") |>
  ggplot(aes(height)) +
  geom_density(fill="#F8766D") 
p2 <- heights |> filter(sex == "Female") |> 
  ggplot(aes(sample=scale(height))) +
  geom_qq() + geom_abline() + ylab("Standard Units")
grid.arrange(p1, p2, ncol=2)
```

## Case study: describing student heights 

* We see something we did not see for the males: the density plot has a second _bump_. 

* Also, the QQ-plot shows that the highest points tend to be taller than expected by the normal distribution. 

* Finally,  we also see five points in the QQ-plot that suggest shorter than expected heights for a normal distribution. 

## Case study: describing student heights 

Regarding the five smallest values, note that these values are:
```{r}
heights |> filter(sex == "Female") |> 
  top_n(5, desc(height)) |>
  pull(height)
```

Because these are reported heights, a possibility is that the student meant to enter `5'1"`, `5'2"`, `5'3"` or `5'5"`. 

## Exercises
